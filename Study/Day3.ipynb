{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcadb057-e0a1-497c-a5c4-acae4122d7f0",
   "metadata": {},
   "source": [
    "## Chapter 4 (p.125 ~ p.180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed5796-5ba8-4126-b4ae-572c73e4f53c",
   "metadata": {},
   "source": [
    "### [1] 퍼셉트론\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67559d5d-c6b0-4beb-9596-604dc1756e91",
   "metadata": {},
   "source": [
    "#### [1-1] 가중 합 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05169756-31fa-4255-93d0-0a329d6a6af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db5f986-1787-4079-9609-ddd36eaa1d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weight(x, w):\n",
    "    return sum(x * w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b30c263-d4f9-454a-942f-c04da1fdbee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = np.array([-0.5, 1, 1])\n",
    "x = np.array([1, 0, 1])\n",
    "\n",
    "add_weight(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d03c540b-fcc5-48fd-91ff-40213ab2c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weight(x, w):\n",
    "    return np.sum(x * w, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5a0a649-cd48-4418-8312-7d7e7b35e15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5,  0.5,  0.5,  1.5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
    "\n",
    "add_weight(x, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f28ce2-bf5b-47e2-b39e-29d49c7d3721",
   "metadata": {},
   "source": [
    "#### [1-2] Sklearn 이용한 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "554bbdad-c45b-4af5-8725-30a793628301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3923b6a5-ed61-4463-9a10-02dac95c05f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습된 퍼셉트론 매개변수 :  [[2. 2.]] [-1.]\n",
      "예측 :  [-1  1  1  1]\n",
      "정확도 :  100.0\n"
     ]
    }
   ],
   "source": [
    "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y = [-1, 1, 1, 1]\n",
    "\n",
    "p = Perceptron()\n",
    "p.fit(X,y)\n",
    "\n",
    "print(\"학습된 퍼셉트론 매개변수 : \", p.coef_, p.intercept_)\n",
    "print(\"예측 : \", p.predict(X))\n",
    "print(\"정확도 : \", p.score(X, y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b4fe36-210e-4451-b1c6-45d033733f56",
   "metadata": {},
   "source": [
    "#### [1-3] 필기 숫자에 퍼셉트론 적용\n",
    "\n",
    "* 숫자 맞추기 => classification => 혼동 행렬 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daccb805-d81e-4802-9506-9dc77c54366d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "혼동 행렬 :  [[48.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. 53.  0.  0.  0.  0.  0.  0.  2.  1.]\n",
      " [ 0.  0. 50.  0.  0.  0.  1.  0.  1.  0.]\n",
      " [ 0.  0.  2. 50.  0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  0.  0.  0. 46.  0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  0.  1.  0. 60.  1.  1.  1.  1.]\n",
      " [ 0.  1.  0.  0.  0.  1. 54.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0. 54.  0.  0.]\n",
      " [ 0.  3.  0.  2.  3.  0.  0.  0. 47.  1.]\n",
      " [ 0.  0.  0.  0.  0.  2.  0.  1.  0. 47.]]\n",
      "accuracy :  94.25925925925925\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# [1] 먼저 dataset load하기\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# [2] train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(digits.data, digits.target, train_size=0.7)\n",
    "\n",
    "# [3] 모델 설정 및 학습\n",
    "p = Perceptron()\n",
    "p.fit(X_train, y_train)\n",
    "\n",
    "# [4] 모델 예측\n",
    "pred = p.predict(X_val)\n",
    "\n",
    "# [5] 혼동 행렬\n",
    "conf = np.zeros((10, 10)) # 숫자는 0 ~ 9\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    conf[pred[i]][y_val[i]] += 1\n",
    "\n",
    "print(\"혼동 행렬 : \", conf)\n",
    "\n",
    "# [5] 성능 측정\n",
    "accuracy = 0\n",
    "for i in range(10):\n",
    "    accuracy += conf[i][i]\n",
    "print(\"accuracy : \", accuracy/len(pred)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a02d381-0e4b-4786-9d0b-0fe7781493e0",
   "metadata": {},
   "source": [
    "### [2] 다층 퍼셉트론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff43280-546a-4568-b887-fc661b666e92",
   "metadata": {},
   "source": [
    "#### [2-1] 필기 숫자에 다층 퍼셉트론 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c57e0c-6ae9-45b0-915a-abadeb274f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc92cc45-a9d7-4038-afe7-997afa9c99f5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.99235077\n",
      "Iteration 2, loss = 0.28886256\n",
      "Iteration 3, loss = 0.19123033\n",
      "Iteration 4, loss = 0.14703939\n",
      "Iteration 5, loss = 0.12514432\n",
      "Iteration 6, loss = 0.09983180\n",
      "Iteration 7, loss = 0.08986335\n",
      "Iteration 8, loss = 0.07664083\n",
      "Iteration 9, loss = 0.06818494\n",
      "Iteration 10, loss = 0.06195242\n",
      "Iteration 11, loss = 0.05630108\n",
      "Iteration 12, loss = 0.05317478\n",
      "Iteration 13, loss = 0.04920228\n",
      "Iteration 14, loss = 0.04424787\n",
      "Iteration 15, loss = 0.04117582\n",
      "Iteration 16, loss = 0.03727866\n",
      "Iteration 17, loss = 0.03522711\n",
      "Iteration 18, loss = 0.03422395\n",
      "Iteration 19, loss = 0.03118713\n",
      "Iteration 20, loss = 0.03051381\n",
      "Iteration 21, loss = 0.03014096\n",
      "Iteration 22, loss = 0.02668531\n",
      "Iteration 23, loss = 0.02523074\n",
      "Iteration 24, loss = 0.02380869\n",
      "Iteration 25, loss = 0.02401279\n",
      "Iteration 26, loss = 0.02229031\n",
      "Iteration 27, loss = 0.02123537\n",
      "Iteration 28, loss = 0.01982369\n",
      "Iteration 29, loss = 0.01963795\n",
      "Iteration 30, loss = 0.01894697\n",
      "Iteration 31, loss = 0.01823649\n",
      "Iteration 32, loss = 0.01781427\n",
      "Iteration 33, loss = 0.01700797\n",
      "Iteration 34, loss = 0.01662918\n",
      "Iteration 35, loss = 0.01569211\n",
      "Iteration 36, loss = 0.01503085\n",
      "Iteration 37, loss = 0.01440623\n",
      "Iteration 38, loss = 0.01433568\n",
      "Iteration 39, loss = 0.01411869\n",
      "Iteration 40, loss = 0.01349174\n",
      "Iteration 41, loss = 0.01260465\n",
      "Iteration 42, loss = 0.01281712\n",
      "Iteration 43, loss = 0.01237011\n",
      "Iteration 44, loss = 0.01211134\n",
      "Iteration 45, loss = 0.01174048\n",
      "Iteration 46, loss = 0.01124893\n",
      "Iteration 47, loss = 0.01106010\n",
      "Iteration 48, loss = 0.01076339\n",
      "Iteration 49, loss = 0.01117708\n",
      "Iteration 50, loss = 0.01032545\n",
      "Iteration 51, loss = 0.01035983\n",
      "Iteration 52, loss = 0.00974592\n",
      "Iteration 53, loss = 0.01008842\n",
      "Iteration 54, loss = 0.00967020\n",
      "Iteration 55, loss = 0.00927661\n",
      "Iteration 56, loss = 0.00913704\n",
      "Iteration 57, loss = 0.00918004\n",
      "Iteration 58, loss = 0.00948937\n",
      "Iteration 59, loss = 0.00863838\n",
      "Iteration 60, loss = 0.00844836\n",
      "Iteration 61, loss = 0.00816715\n",
      "Iteration 62, loss = 0.00818232\n",
      "Iteration 63, loss = 0.00799092\n",
      "Iteration 64, loss = 0.00808852\n",
      "Iteration 65, loss = 0.00760803\n",
      "Iteration 66, loss = 0.00748683\n",
      "Iteration 67, loss = 0.00744852\n",
      "Iteration 68, loss = 0.00726380\n",
      "Iteration 69, loss = 0.00716480\n",
      "Iteration 70, loss = 0.00698817\n",
      "Iteration 71, loss = 0.00688983\n",
      "Iteration 72, loss = 0.00683241\n",
      "Iteration 73, loss = 0.00667278\n",
      "Iteration 74, loss = 0.00659537\n",
      "Iteration 75, loss = 0.00658912\n",
      "Iteration 76, loss = 0.00640098\n",
      "Iteration 77, loss = 0.00634343\n",
      "Iteration 78, loss = 0.00637297\n",
      "Iteration 79, loss = 0.00615925\n",
      "Iteration 80, loss = 0.00609165\n",
      "Iteration 81, loss = 0.00601405\n",
      "Iteration 82, loss = 0.00608309\n",
      "Iteration 83, loss = 0.00592826\n",
      "Iteration 84, loss = 0.00577428\n",
      "Iteration 85, loss = 0.00569325\n",
      "Iteration 86, loss = 0.00563130\n",
      "Iteration 87, loss = 0.00557949\n",
      "Iteration 88, loss = 0.00553939\n",
      "Iteration 89, loss = 0.00540437\n",
      "Iteration 90, loss = 0.00542763\n",
      "Iteration 91, loss = 0.00530827\n",
      "Iteration 92, loss = 0.00518316\n",
      "Iteration 93, loss = 0.00523842\n",
      "Iteration 94, loss = 0.00509848\n",
      "Iteration 95, loss = 0.00507819\n",
      "Iteration 96, loss = 0.00506245\n",
      "Iteration 97, loss = 0.00497909\n",
      "Iteration 98, loss = 0.00488075\n",
      "Iteration 99, loss = 0.00484669\n",
      "Iteration 100, loss = 0.00477247\n",
      "Iteration 101, loss = 0.00474867\n",
      "Iteration 102, loss = 0.00467228\n",
      "Iteration 103, loss = 0.00463896\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "혼동 행렬 :  [[61.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. 53.  1.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0. 55.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. 53.  0.  1.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0. 44.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.  0. 51.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  1. 49.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0. 57.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0. 50.  0.]\n",
      " [ 0.  0.  0.  0.  0.  2.  0.  0.  0. 52.]]\n",
      "accuracy :  97.22222222222221\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# [1] 먼저 dataset load하기\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# [2] train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(digits.data, digits.target, train_size=0.7)\n",
    "\n",
    "# [3] 모델 설정 및 학습\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes = (100),\n",
    "    learning_rate_init = 0.001,\n",
    "    batch_size = 32, \n",
    "    max_iter = 300,\n",
    "    solver = 'sgd', # Stochastic Gradient Descent => 한 번의 파라미터 업데이트를 위해 하나의 훈련 데이터 사용\n",
    "    verbose = True\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# [4] 모델 예측\n",
    "pred = mlp.predict(X_val)\n",
    "\n",
    "# [5] 혼동 행렬\n",
    "conf = np.zeros((10, 10)) # 숫자는 0 ~ 9\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    conf[pred[i]][y_val[i]] += 1\n",
    "\n",
    "print(\"혼동 행렬 : \", conf)\n",
    "\n",
    "# [5] 성능 측정\n",
    "accuracy = 0\n",
    "for i in range(10):\n",
    "    accuracy += conf[i][i]\n",
    "print(\"accuracy : \", accuracy/len(pred)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1441d8a-fe2b-4725-99aa-f0030f64d356",
   "metadata": {},
   "source": [
    "#### [2-2] MNIST에 다층 퍼셉트론 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be689b4-53bf-48ac-ad3a-a609c70d64da",
   "metadata": {},
   "source": [
    "* 기존 데이터 셋 : 샘플 1797개, 8*8 데이터 맵\n",
    "* MNIST : 샘플 70000개, 28*28 데이터 맵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e91975b-993b-4535-a583-b2c919de725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69ebc4ac-5fa8-4349-a132-3e13e42711b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] data load\n",
    "mnist = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ee30443-51d3-4239-8318-985a46c40fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.data = mnist.data/255.0\n",
    "X_train = mnist.data[:60000]\n",
    "X_val = mnist.data[60000:]\n",
    "\n",
    "y_train =np.int16(mnist.target[:60000])\n",
    "y_val = np.int16(mnist.target[60000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f9b2b68-f46e-4245-9d81-ca1a88080c74",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.92291204\n",
      "Iteration 2, loss = 0.42965271\n",
      "Iteration 3, loss = 0.36342151\n",
      "Iteration 4, loss = 0.33006809\n",
      "Iteration 5, loss = 0.30727047\n",
      "Iteration 6, loss = 0.28973820\n",
      "Iteration 7, loss = 0.27505869\n",
      "Iteration 8, loss = 0.26223752\n",
      "Iteration 9, loss = 0.25115236\n",
      "Iteration 10, loss = 0.24113923\n",
      "Iteration 11, loss = 0.23205889\n",
      "Iteration 12, loss = 0.22382421\n",
      "Iteration 13, loss = 0.21608971\n",
      "Iteration 14, loss = 0.20901141\n",
      "Iteration 15, loss = 0.20245036\n",
      "Iteration 16, loss = 0.19640533\n",
      "Iteration 17, loss = 0.19071948\n",
      "Iteration 18, loss = 0.18494821\n",
      "Iteration 19, loss = 0.17983288\n",
      "Iteration 20, loss = 0.17509667\n",
      "Iteration 21, loss = 0.17047378\n",
      "Iteration 22, loss = 0.16594587\n",
      "Iteration 23, loss = 0.16186649\n",
      "Iteration 24, loss = 0.15797090\n",
      "Iteration 25, loss = 0.15406724\n",
      "Iteration 26, loss = 0.15047790\n",
      "Iteration 27, loss = 0.14704297\n",
      "Iteration 28, loss = 0.14381944\n",
      "Iteration 29, loss = 0.14058989\n",
      "Iteration 30, loss = 0.13751323\n",
      "Iteration 31, loss = 0.13463819\n",
      "Iteration 32, loss = 0.13173352\n",
      "Iteration 33, loss = 0.12916358\n",
      "Iteration 34, loss = 0.12654856\n",
      "Iteration 35, loss = 0.12404877\n",
      "Iteration 36, loss = 0.12169655\n",
      "Iteration 37, loss = 0.11931193\n",
      "Iteration 38, loss = 0.11711233\n",
      "Iteration 39, loss = 0.11493173\n",
      "Iteration 40, loss = 0.11283649\n",
      "Iteration 41, loss = 0.11095593\n",
      "Iteration 42, loss = 0.10889679\n",
      "Iteration 43, loss = 0.10705083\n",
      "Iteration 44, loss = 0.10511700\n",
      "Iteration 45, loss = 0.10348393\n",
      "Iteration 46, loss = 0.10175548\n",
      "Iteration 47, loss = 0.10024251\n",
      "Iteration 48, loss = 0.09847342\n",
      "Iteration 49, loss = 0.09699787\n",
      "Iteration 50, loss = 0.09545299\n",
      "Iteration 51, loss = 0.09398161\n",
      "Iteration 52, loss = 0.09258294\n",
      "Iteration 53, loss = 0.09129838\n",
      "Iteration 54, loss = 0.08992569\n",
      "Iteration 55, loss = 0.08866822\n",
      "Iteration 56, loss = 0.08739375\n",
      "Iteration 57, loss = 0.08623931\n",
      "Iteration 58, loss = 0.08504745\n",
      "Iteration 59, loss = 0.08379191\n",
      "Iteration 60, loss = 0.08260361\n",
      "Iteration 61, loss = 0.08159110\n",
      "Iteration 62, loss = 0.08043089\n",
      "Iteration 63, loss = 0.07943911\n",
      "Iteration 64, loss = 0.07839645\n",
      "Iteration 65, loss = 0.07739845\n",
      "Iteration 66, loss = 0.07632945\n",
      "Iteration 67, loss = 0.07552352\n",
      "Iteration 68, loss = 0.07453737\n",
      "Iteration 69, loss = 0.07363038\n",
      "Iteration 70, loss = 0.07267909\n",
      "Iteration 71, loss = 0.07185400\n",
      "Iteration 72, loss = 0.07096130\n",
      "Iteration 73, loss = 0.07023125\n",
      "Iteration 74, loss = 0.06934334\n",
      "Iteration 75, loss = 0.06847897\n",
      "Iteration 76, loss = 0.06773681\n",
      "Iteration 77, loss = 0.06703501\n",
      "Iteration 78, loss = 0.06621842\n",
      "Iteration 79, loss = 0.06553710\n",
      "Iteration 80, loss = 0.06474345\n",
      "Iteration 81, loss = 0.06400958\n",
      "Iteration 82, loss = 0.06330465\n",
      "Iteration 83, loss = 0.06263434\n",
      "Iteration 84, loss = 0.06199281\n",
      "Iteration 85, loss = 0.06128635\n",
      "Iteration 86, loss = 0.06065559\n",
      "Iteration 87, loss = 0.06006660\n",
      "Iteration 88, loss = 0.05942204\n",
      "Iteration 89, loss = 0.05883688\n",
      "Iteration 90, loss = 0.05816801\n",
      "Iteration 91, loss = 0.05761930\n",
      "Iteration 92, loss = 0.05700441\n",
      "Iteration 93, loss = 0.05639287\n",
      "Iteration 94, loss = 0.05586553\n",
      "Iteration 95, loss = 0.05534055\n",
      "Iteration 96, loss = 0.05470276\n",
      "Iteration 97, loss = 0.05420478\n",
      "Iteration 98, loss = 0.05360510\n",
      "Iteration 99, loss = 0.05311342\n",
      "Iteration 100, loss = 0.05269092\n",
      "Iteration 101, loss = 0.05221245\n",
      "Iteration 102, loss = 0.05168478\n",
      "Iteration 103, loss = 0.05117535\n",
      "Iteration 104, loss = 0.05065652\n",
      "Iteration 105, loss = 0.05021428\n",
      "Iteration 106, loss = 0.04968321\n",
      "Iteration 107, loss = 0.04932093\n",
      "Iteration 108, loss = 0.04884660\n",
      "Iteration 109, loss = 0.04844004\n",
      "Iteration 110, loss = 0.04787686\n",
      "Iteration 111, loss = 0.04752115\n",
      "Iteration 112, loss = 0.04703651\n",
      "Iteration 113, loss = 0.04658960\n",
      "Iteration 114, loss = 0.04625317\n",
      "Iteration 115, loss = 0.04586049\n",
      "Iteration 116, loss = 0.04540332\n",
      "Iteration 117, loss = 0.04504257\n",
      "Iteration 118, loss = 0.04466612\n",
      "Iteration 119, loss = 0.04422671\n",
      "Iteration 120, loss = 0.04392341\n",
      "Iteration 121, loss = 0.04344514\n",
      "Iteration 122, loss = 0.04307525\n",
      "Iteration 123, loss = 0.04268659\n",
      "Iteration 124, loss = 0.04228724\n",
      "Iteration 125, loss = 0.04201123\n",
      "Iteration 126, loss = 0.04166249\n",
      "Iteration 127, loss = 0.04129912\n",
      "Iteration 128, loss = 0.04091868\n",
      "Iteration 129, loss = 0.04052711\n",
      "Iteration 130, loss = 0.04023410\n",
      "Iteration 131, loss = 0.03990342\n",
      "Iteration 132, loss = 0.03962782\n",
      "Iteration 133, loss = 0.03927127\n",
      "Iteration 134, loss = 0.03894419\n",
      "Iteration 135, loss = 0.03862115\n",
      "Iteration 136, loss = 0.03830435\n",
      "Iteration 137, loss = 0.03801079\n",
      "Iteration 138, loss = 0.03764742\n",
      "Iteration 139, loss = 0.03740744\n",
      "Iteration 140, loss = 0.03708497\n",
      "Iteration 141, loss = 0.03678389\n",
      "Iteration 142, loss = 0.03653739\n",
      "Iteration 143, loss = 0.03624384\n",
      "Iteration 144, loss = 0.03584149\n",
      "Iteration 145, loss = 0.03563944\n",
      "Iteration 146, loss = 0.03533209\n",
      "Iteration 147, loss = 0.03501127\n",
      "Iteration 148, loss = 0.03480911\n",
      "Iteration 149, loss = 0.03451005\n",
      "Iteration 150, loss = 0.03422699\n",
      "Iteration 151, loss = 0.03406217\n",
      "Iteration 152, loss = 0.03373218\n",
      "Iteration 153, loss = 0.03349413\n",
      "Iteration 154, loss = 0.03324809\n",
      "Iteration 155, loss = 0.03293986\n",
      "Iteration 156, loss = 0.03272304\n",
      "Iteration 157, loss = 0.03247542\n",
      "Iteration 158, loss = 0.03221653\n",
      "Iteration 159, loss = 0.03195992\n",
      "Iteration 160, loss = 0.03172491\n",
      "Iteration 161, loss = 0.03150101\n",
      "Iteration 162, loss = 0.03122960\n",
      "Iteration 163, loss = 0.03102867\n",
      "Iteration 164, loss = 0.03076865\n",
      "Iteration 165, loss = 0.03054780\n",
      "Iteration 166, loss = 0.03030406\n",
      "Iteration 167, loss = 0.03007121\n",
      "Iteration 168, loss = 0.02989924\n",
      "Iteration 169, loss = 0.02964306\n",
      "Iteration 170, loss = 0.02948127\n",
      "Iteration 171, loss = 0.02923377\n",
      "Iteration 172, loss = 0.02901291\n",
      "Iteration 173, loss = 0.02884288\n",
      "Iteration 174, loss = 0.02857342\n",
      "Iteration 175, loss = 0.02835763\n",
      "Iteration 176, loss = 0.02813475\n",
      "Iteration 177, loss = 0.02795689\n",
      "Iteration 178, loss = 0.02774828\n",
      "Iteration 179, loss = 0.02759708\n",
      "Iteration 180, loss = 0.02739254\n",
      "Iteration 181, loss = 0.02719581\n",
      "Iteration 182, loss = 0.02699456\n",
      "Iteration 183, loss = 0.02679078\n",
      "Iteration 184, loss = 0.02663366\n",
      "Iteration 185, loss = 0.02644729\n",
      "Iteration 186, loss = 0.02620475\n",
      "Iteration 187, loss = 0.02607003\n",
      "Iteration 188, loss = 0.02588522\n",
      "Iteration 189, loss = 0.02568755\n",
      "Iteration 190, loss = 0.02550646\n",
      "Iteration 191, loss = 0.02530251\n",
      "Iteration 192, loss = 0.02515580\n",
      "Iteration 193, loss = 0.02497627\n",
      "Iteration 194, loss = 0.02482459\n",
      "Iteration 195, loss = 0.02470045\n",
      "Iteration 196, loss = 0.02446968\n",
      "Iteration 197, loss = 0.02433549\n",
      "Iteration 198, loss = 0.02415843\n",
      "Iteration 199, loss = 0.02397552\n",
      "Iteration 200, loss = 0.02380389\n",
      "Iteration 201, loss = 0.02365295\n",
      "Iteration 202, loss = 0.02350520\n",
      "Iteration 203, loss = 0.02336008\n",
      "Iteration 204, loss = 0.02318308\n",
      "Iteration 205, loss = 0.02304802\n",
      "Iteration 206, loss = 0.02287337\n",
      "Iteration 207, loss = 0.02276823\n",
      "Iteration 208, loss = 0.02257738\n",
      "Iteration 209, loss = 0.02244978\n",
      "Iteration 210, loss = 0.02228858\n",
      "Iteration 211, loss = 0.02214795\n",
      "Iteration 212, loss = 0.02203018\n",
      "Iteration 213, loss = 0.02184163\n",
      "Iteration 214, loss = 0.02172156\n",
      "Iteration 215, loss = 0.02151199\n",
      "Iteration 216, loss = 0.02143745\n",
      "Iteration 217, loss = 0.02128160\n",
      "Iteration 218, loss = 0.02115939\n",
      "Iteration 219, loss = 0.02101210\n",
      "Iteration 220, loss = 0.02087878\n",
      "Iteration 221, loss = 0.02068714\n",
      "Iteration 222, loss = 0.02061854\n",
      "Iteration 223, loss = 0.02050158\n",
      "Iteration 224, loss = 0.02038186\n",
      "Iteration 225, loss = 0.02021604\n",
      "Iteration 226, loss = 0.02008478\n",
      "Iteration 227, loss = 0.02000483\n",
      "Iteration 228, loss = 0.01986281\n",
      "Iteration 229, loss = 0.01973957\n",
      "Iteration 230, loss = 0.01959325\n",
      "Iteration 231, loss = 0.01947064\n",
      "Iteration 232, loss = 0.01937320\n",
      "Iteration 233, loss = 0.01924856\n",
      "Iteration 234, loss = 0.01913459\n",
      "Iteration 235, loss = 0.01902482\n",
      "Iteration 236, loss = 0.01892812\n",
      "Iteration 237, loss = 0.01879394\n",
      "Iteration 238, loss = 0.01866262\n",
      "Iteration 239, loss = 0.01856427\n",
      "Iteration 240, loss = 0.01843593\n",
      "Iteration 241, loss = 0.01834918\n",
      "Iteration 242, loss = 0.01821089\n",
      "Iteration 243, loss = 0.01811434\n",
      "Iteration 244, loss = 0.01803058\n",
      "Iteration 245, loss = 0.01790203\n",
      "Iteration 246, loss = 0.01782264\n",
      "Iteration 247, loss = 0.01767444\n",
      "Iteration 248, loss = 0.01761425\n",
      "Iteration 249, loss = 0.01749863\n",
      "Iteration 250, loss = 0.01737916\n",
      "Iteration 251, loss = 0.01725450\n",
      "Iteration 252, loss = 0.01717454\n",
      "Iteration 253, loss = 0.01707630\n",
      "Iteration 254, loss = 0.01695234\n",
      "Iteration 255, loss = 0.01689342\n",
      "Iteration 256, loss = 0.01675278\n",
      "Iteration 257, loss = 0.01670179\n",
      "Iteration 258, loss = 0.01657724\n",
      "Iteration 259, loss = 0.01650181\n",
      "Iteration 260, loss = 0.01643295\n",
      "Iteration 261, loss = 0.01630193\n",
      "Iteration 262, loss = 0.01618864\n",
      "Iteration 263, loss = 0.01611586\n",
      "Iteration 264, loss = 0.01607039\n",
      "Iteration 265, loss = 0.01593785\n",
      "Iteration 266, loss = 0.01584824\n",
      "Iteration 267, loss = 0.01578239\n",
      "Iteration 268, loss = 0.01564533\n",
      "Iteration 269, loss = 0.01559954\n",
      "Iteration 270, loss = 0.01550997\n",
      "Iteration 271, loss = 0.01540551\n",
      "Iteration 272, loss = 0.01531680\n",
      "Iteration 273, loss = 0.01529073\n",
      "Iteration 274, loss = 0.01517057\n",
      "Iteration 275, loss = 0.01508691\n",
      "Iteration 276, loss = 0.01501358\n",
      "Iteration 277, loss = 0.01491126\n",
      "Iteration 278, loss = 0.01482131\n",
      "Iteration 279, loss = 0.01477467\n",
      "Iteration 280, loss = 0.01468907\n",
      "Iteration 281, loss = 0.01459849\n",
      "Iteration 282, loss = 0.01452536\n",
      "Iteration 283, loss = 0.01444772\n",
      "Iteration 284, loss = 0.01434556\n",
      "Iteration 285, loss = 0.01428626\n",
      "Iteration 286, loss = 0.01421094\n",
      "Iteration 287, loss = 0.01410855\n",
      "Iteration 288, loss = 0.01404082\n",
      "Iteration 289, loss = 0.01397792\n",
      "Iteration 290, loss = 0.01391230\n",
      "Iteration 291, loss = 0.01383628\n",
      "Iteration 292, loss = 0.01377578\n",
      "Iteration 293, loss = 0.01370350\n",
      "Iteration 294, loss = 0.01362218\n",
      "Iteration 295, loss = 0.01357042\n",
      "Iteration 296, loss = 0.01347669\n",
      "Iteration 297, loss = 0.01342992\n",
      "Iteration 298, loss = 0.01335004\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# [3] 모델 설정 및 학습\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes = (100),\n",
    "    learning_rate_init = 0.001,\n",
    "    batch_size = 64, \n",
    "    max_iter = 300,\n",
    "    activation='relu',\n",
    "    solver = 'sgd', # Stochastic Gradient Descent => 한 번의 파라미터 업데이트를 위해 하나의 훈련 데이터 사용\n",
    "    verbose = True\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# [4] 모델 예측\n",
    "pred = mlp.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ed96686-4bce-42f5-a351-d43af8b01099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "혼동 행렬 :  [[ 966    0    2    0    2    2    4    1    4    1]\n",
      " [   0 1121    2    0    0    0    3    5    0    3]\n",
      " [   1    3 1008    3    2    0    2    6    4    0]\n",
      " [   1    1    3  990    1   10    1    4    8    4]\n",
      " [   0    0    3    2  960    1    2    2    2    6]\n",
      " [   3    1    0    2    0  864    4    0    4    2]\n",
      " [   5    3    3    0    3    7  940    0    2    1]\n",
      " [   1    1    5    2    2    0    1 1003    4    2]\n",
      " [   2    5    6    4    0    6    1    0  944    2]\n",
      " [   1    0    0    7   12    2    0    7    2  988]]\n",
      "accuracy :  97.84\n"
     ]
    }
   ],
   "source": [
    "# [5] 혼동 행렬\n",
    "conf = np.zeros((10, 10), dtype=np.int16) # 숫자는 0 ~ 9\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    conf[pred[i]][y_val[i]] += 1\n",
    "\n",
    "print(\"혼동 행렬 : \", conf)\n",
    "\n",
    "# [5] 성능 측정\n",
    "accuracy = 0\n",
    "for i in range(10):\n",
    "    accuracy += conf[i][i]\n",
    "print(\"accuracy : \", accuracy/len(pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7c88f8-b744-4445-bfd8-3a0f211b5426",
   "metadata": {},
   "source": [
    "## [3] 하이퍼 매개변수 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596932d1-4ad7-4972-8cec-8a309c7f37cc",
   "metadata": {},
   "source": [
    "#### [3-1] 단일 하이퍼 매개변수 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96e08cf-a3fe-4340-af86-f0655bc608dc",
   "metadata": {},
   "source": [
    "* validation_curve 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3aa109e-5ea8-4d80-a319-0fe5cdffec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, validation_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd728cf8-269a-4e6b-9cb3-595dfa883ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] 데이터 셋 불러오기\n",
    "digit = datasets.load_digits()\n",
    "x_train, x_test, y_train, y_test = train_test_split(digit.data, digit.target, train_size=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d68d3f5-8e5f-46b2-a176-8742be583f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하이퍼 매개변수 최적화 걸린 시간 ;  146.91457295417786\n"
     ]
    }
   ],
   "source": [
    "# [2] 다층 퍼셉트론을 교차 검증으로 성능 평가\n",
    "start = time.time()\n",
    "mlp = MLPClassifier(\n",
    "    learning_rate_init=0.001,\n",
    "    batch_size = 32,\n",
    "    max_iter=300,\n",
    "    solver='sgd'\n",
    ")\n",
    "prange = range(50, 1001, 50)\n",
    "train_score, test_score = validation_curve(\n",
    "    mlp, x_train, y_train, param_name = 'hidden_layer_sizes',\n",
    "    param_range=prange, cv=10,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=4\n",
    ")\n",
    "end = time.time()\n",
    "print(\"하이퍼 매개변수 최적화 걸린 시간 ; \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b03844f5-0d93-468d-a011-51609975df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] 교차 검증 결과의 평균과 분산 구하기\n",
    "train_mean = np.mean(train_score, axis=1)\n",
    "train_std = np.std(train_score, axis=1)\n",
    "test_mean = np.mean(test_score, axis=1)\n",
    "test_std = np.std(test_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bc1f4c0-2cfa-4220-ae4a-1a598921926e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [4] 성능 그래프\n",
    "plt.plot(prange, train_mean, label=\"Train Score\", color=\"r\")\n",
    "plt.plot(prange, test_mean, label=\"Test Score\", color=\"b\")\n",
    "\n",
    "plt.fill_between(prange, train_mean - train_std, train_mean+train_std, alpha=0.2, color=\"r\")\n",
    "plt.fill_between(prange, test_mean - test_std, test_mean+test_std, alpha=0.2, color=\"b\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Validation of hidden nodes\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0.9, 1.01)\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ab94bec-2194-4f49-b657-291bffcb0d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 히든 노드 개수 :  650\n"
     ]
    }
   ],
   "source": [
    "best_number_nodes = prange[np.argmax(test_mean)]\n",
    "print('최적의 히든 노드 개수 : ', best_number_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d3efc72-b9c2-4fe8-94d2-873526a55ddd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.98854420\n",
      "Iteration 2, loss = 0.15014352\n",
      "Iteration 3, loss = 0.09569034\n",
      "Iteration 4, loss = 0.07070479\n",
      "Iteration 5, loss = 0.05807992\n",
      "Iteration 6, loss = 0.05233018\n",
      "Iteration 7, loss = 0.04424900\n",
      "Iteration 8, loss = 0.03899632\n",
      "Iteration 9, loss = 0.03481230\n",
      "Iteration 10, loss = 0.03005561\n",
      "Iteration 11, loss = 0.03134834\n",
      "Iteration 12, loss = 0.02696145\n",
      "Iteration 13, loss = 0.02517628\n",
      "Iteration 14, loss = 0.02218653\n",
      "Iteration 15, loss = 0.02070481\n",
      "Iteration 16, loss = 0.02270437\n",
      "Iteration 17, loss = 0.01826513\n",
      "Iteration 18, loss = 0.01943758\n",
      "Iteration 19, loss = 0.01697456\n",
      "Iteration 20, loss = 0.01628660\n",
      "Iteration 21, loss = 0.01495094\n",
      "Iteration 22, loss = 0.01525563\n",
      "Iteration 23, loss = 0.01399552\n",
      "Iteration 24, loss = 0.01299555\n",
      "Iteration 25, loss = 0.01346369\n",
      "Iteration 26, loss = 0.01287884\n",
      "Iteration 27, loss = 0.01194657\n",
      "Iteration 28, loss = 0.01144062\n",
      "Iteration 29, loss = 0.01098056\n",
      "Iteration 30, loss = 0.01147946\n",
      "Iteration 31, loss = 0.01039280\n",
      "Iteration 32, loss = 0.00997207\n",
      "Iteration 33, loss = 0.00975438\n",
      "Iteration 34, loss = 0.00946358\n",
      "Iteration 35, loss = 0.00931915\n",
      "Iteration 36, loss = 0.00890945\n",
      "Iteration 37, loss = 0.00894675\n",
      "Iteration 38, loss = 0.00845726\n",
      "Iteration 39, loss = 0.00847807\n",
      "Iteration 40, loss = 0.00804636\n",
      "Iteration 41, loss = 0.00777363\n",
      "Iteration 42, loss = 0.00786844\n",
      "Iteration 43, loss = 0.00744112\n",
      "Iteration 44, loss = 0.00744400\n",
      "Iteration 45, loss = 0.00719295\n",
      "Iteration 46, loss = 0.00694420\n",
      "Iteration 47, loss = 0.00679341\n",
      "Iteration 48, loss = 0.00678820\n",
      "Iteration 49, loss = 0.00659805\n",
      "Iteration 50, loss = 0.00649753\n",
      "Iteration 51, loss = 0.00636011\n",
      "Iteration 52, loss = 0.00622812\n",
      "Iteration 53, loss = 0.00653044\n",
      "Iteration 54, loss = 0.00605390\n",
      "Iteration 55, loss = 0.00582958\n",
      "Iteration 56, loss = 0.00579085\n",
      "Iteration 57, loss = 0.00566186\n",
      "Iteration 58, loss = 0.00559340\n",
      "Iteration 59, loss = 0.00539167\n",
      "Iteration 60, loss = 0.00543248\n",
      "Iteration 61, loss = 0.00531057\n",
      "Iteration 62, loss = 0.00521187\n",
      "Iteration 63, loss = 0.00514957\n",
      "Iteration 64, loss = 0.00504956\n",
      "Iteration 65, loss = 0.00505543\n",
      "Iteration 66, loss = 0.00495969\n",
      "Iteration 67, loss = 0.00480648\n",
      "Iteration 68, loss = 0.00477308\n",
      "Iteration 69, loss = 0.00474276\n",
      "Iteration 70, loss = 0.00467240\n",
      "Iteration 71, loss = 0.00455178\n",
      "Iteration 72, loss = 0.00451809\n",
      "Iteration 73, loss = 0.00440795\n",
      "Iteration 74, loss = 0.00438854\n",
      "Iteration 75, loss = 0.00431692\n",
      "Iteration 76, loss = 0.00426800\n",
      "Iteration 77, loss = 0.00423003\n",
      "Iteration 78, loss = 0.00418779\n",
      "Iteration 79, loss = 0.00415976\n",
      "Iteration 80, loss = 0.00408818\n",
      "Iteration 81, loss = 0.00402530\n",
      "Iteration 82, loss = 0.00399333\n",
      "Iteration 83, loss = 0.00392384\n",
      "Iteration 84, loss = 0.00386497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=32, hidden_layer_sizes=650, max_iter=300, solver='sgd',\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [5] 최적의 히든 노드 개수로 모델링 및 학습\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes = best_number_nodes,\n",
    "    learning_rate_init = 0.001,\n",
    "    batch_size = 32, \n",
    "    max_iter = 300,\n",
    "    solver = 'sgd', # Stochastic Gradient Descent => 한 번의 파라미터 업데이트를 위해 하나의 훈련 데이터 사용\n",
    "    verbose = True\n",
    ")\n",
    "mlp.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dcc8f9e-087d-427d-96b0-c76460c0706a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "혼동 행렬 :  [[61.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0. 46.  1.  0.  0.  0.  0.  0.  4.  0.]\n",
      " [ 0.  0. 53.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. 59.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. 55.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0. 53.  2.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1. 45.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. 52.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0. 43.  2.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  1.  0. 55.]]\n",
      "accuracy :  96.66666666666667\n"
     ]
    }
   ],
   "source": [
    "# [6] 모델 예측\n",
    "pred = mlp.predict(x_test)\n",
    "\n",
    "# [7] 혼동 행렬\n",
    "conf = np.zeros((10, 10)) # 숫자는 0 ~ 9\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    conf[pred[i]][y_test[i]] += 1\n",
    "\n",
    "print(\"혼동 행렬 : \", conf)\n",
    "\n",
    "# [5] 성능 측정\n",
    "accuracy = 0\n",
    "for i in range(10):\n",
    "    accuracy += conf[i][i]\n",
    "print(\"accuracy : \", accuracy/len(pred)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8143e70b-b54c-46de-9a62-239d31356cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56932c-46e9-4377-9834-5018417fd77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
